[TOC]

### 1、调度器和执行流简介

**1.1** 一个处理器任意时刻只能执行一个任务，真正的并行是指多个处理器同时工作，一台计算机的并行能力取决于其物理处理器的数量。

**1.2** 任务调度器是操作系统中用于把任务轮流调度上处理器运行的一个软件模块，它是操作系统的一部分；调度器在内核中维护一个任务表（进程表、线程表或调度表），按照一定调度算法，从任务表中选择一个任务，然后把该任务放到处理器上运行，当任务运行的时间片到期后，再从任务表中找到另外一个任务放到处理器上运行，周而复始，让任务表中的所有任务都有机会运行。有了调度器，所任务操作系统才能得以实现，它是多任务系统的核心，它的好坏直接影响了系统的效率。

**1.3** 执行流对应于代码，大到可以是整个文件，即进程，小到可以是一个功能独立的代码块，即函数，而线程的本质上就是函数。执行流是独立的，它的独立性体现在每个执行流都有自己的栈、一套自己的寄存器映像和内存资源，这是Intel处理器在硬件上规定的，其实这是执行流的上下文环境。

**1.4** 在任务调度器的眼里，只有执行流才是调度单元，即处理器上运行的每个任务都是调度器给分配的执行流，只要成为执行流就能够独立上处理器运行了。

### 2、线程是什么

**2.1** 线程是一套机制，此机制可以为一般的代码块创造它所依赖的上下文环境，从而让代码具有独立性，因此在原理上线程能使一段函数成为调度单元（执行流），使函数能被调度器“认可”，从而能够被专门调度到处理器上执行。

**2.2** 在线程中调用函数是让所运行的函数能够以调度单元的身份独立上处理器运行，当函数可以独立运行时，可让程序中的多个函数并发的方式运行，为程序提速。

### 3、进程与线程的关系

**3.1** 进程是指正在运行的程序，即进行中的程序，程序必须在获得运行所需要的各类资源后才能成为进程，资源包括进程所使用的栈，使用的寄存器等。

**3.2** 对于处理器来说，进程是一种控制流集合，集合中至少包含一条执行流，执行流之间是相互独立的，但它们共享进程的所有资源，它们是处理器的执行单位，或者成为调度单位，它们就是线程。线程是在进程基础之上的二次并发。 

**3.3** 线程和进程比，进程拥有整个地址空间，从而拥有全部资源，线程没有自己的地址空间，因此没有任何属于自己的资源，需要借助进程的资源"生存"，所以线程被称为轻量级进程。

**3.4** 线程是给进程提速的。一是进程采用多个执行流（线程）和其他进程抢占处理器资源，这样就节省了单个进程的总执行时间。二是避免了阻塞整个进程（内核级线程）

**3.5** 进程 = 线程 + 资源

**3.6** 各个进程都拥有自己的虚拟地址空间，正常情况下，它们无法访问到对方的内部，进程间的安全性是由操作系统的分页机制来保证的，操作系统会分配不相同的物理页给多个进程。

**3.7** 线程才具备能动性，它才是处理器的执行单元，因此它是调度器眼中的调度单位。进程只是个资源整合体，它将进程中所有线程运行时用到资源收集在一起，供进程中所有线程使用，真正上处理器运行的其实都叫线程。

### 4、进程、线程的状态

**4.1** 运行态：正在处理器上运行的进程的状态

**4.2** 就绪态：进程可以随时准备运行的状态

**4.3** 阻塞态：需要等待外界条件的状态

**4.4** 进程的状态表示进程从出生到死亡的一系列所处的阶段，操作系统的调度器可以利用它更加高效地管理进程调度

~~~c
/* 进程或线程的状态 */
enum task_status {
   	TASK_RUNNING, 	// 运行态
   	TASK_READY, 	// 就绪态
   	TASK_BLOCKED, 	// 阻塞态
   	TASK_WAITING, 	// 等待
   	TASK_HANGING, 	// 挂起
   	TASK_DIED 		// 终止态
};
~~~

### 5、PCB（进程控制块）

~~~c
/* 进程或线程的pcb,程序控制块 */
struct task_struct {
   	uint32_t* self_kstack;	 	// 各内核线程都用自己的内核栈
   	pid_t pid;
   	enum task_status status;
   	char name[TASK_NAME_LEN];
   	uint8_t priority;
   	uint8_t ticks;	   			// 每次在处理器上执行的时间嘀嗒数
   	uint32_t elapsed_ticks; 	// 此任务自上cpu运行后至今占用了多少嘀嗒数
   	struct list_elem general_tag; 	// 用于线程在一般的队列中的结点			    
   	struct list_elem all_list_tag; 	// 用于线程队列thread_all_list中的结点
   	uint32_t* pgdir;              	// 进程自己页表的虚拟地址
   	struct virtual_addr userprog_vaddr;   			// 用户进程的虚拟地址
   	struct mem_block_desc u_block_desc[DESC_CNT];   // 用户进程内存块描述符
   	int32_t fd_table[MAX_FILES_OPEN_PER_PROC];		// 已打开文件数组
   	uint32_t cwd_inode_nr;	 						// 进程所在的工作目录的inode编号
   	pid_t parent_pid;		 						// 父进程pid
   	int8_t  exit_status;         					// 进程结束时自己调用exit传入的参数
   	uint32_t stack_magic;	 						// 栈的边界标记,用于检测栈的溢出
};
~~~

### 6、 创建线程

**6.1**  线程创建函数 thread_start(name, prio, func, arg), 线程创建完成后执行函数func(arg);
**6.2**  线程需要有自己的栈（保护上下文），有一套寄存器映像和内存资源；创建线程流程：

1. 在内核物理内存池中分配一页内存给线程PCB
2. 通过init_thread函数初始化线程的基本信息（分配pid,线程名称，线程状态，线程内核栈，时间片，文件描述符，父进程，inode编号）
3. 通过thread_create函数初始化线程内核栈
    ① kt_stack->eip = kernel_thread ---> 该函数执行func(arg)
    ② kt_stack->func = func;  kt_stack->arg = arg
4. 将线程PCB加入到就绪队列（thread_ready_list）和所有线程队列（thread_all_list）等待调度器调度

### 7、初始化线程环境

**7.1**  线程PCB在内存中的位置是散落的，需要通过链表将它们串起来，从而形成队列。调度器主要的任务就是读写就绪队列，增删里面的节点，节点是线程PCB中的节点，“相当于线程的PCB”，从队列中将其取出时，可以还原成PCB。故需要先创建线程就绪队列（thread_ready_list）和管理所有线程的全部线程队列（thread_all_list）,并初始化。

![img](https://s2.loli.net/2022/02/14/7VHJizCQ6USwhr3.png)

**7.2** 进程创建时，需要给进程分配进程号pid, 进程销毁时，需要回收，故使用位图管理进程号pid，目前实现最大支持1024个进程并发；初始化pid, btm_bytes_len = (1024/8 = 128),pid_start = 1;

**7.3** 将内核中的main函数设置成主线程；

~~~c
// 将内核中的main函数设置成主线程
// 1. 因为main函数在内核加载器（loader），加载结束后就开始运行了。 栈顶指针esp存的是0xc009f000,预留了PCB的地址0xc009e000,故不需要在分配一页内存做PCB了
// 2. 通过init_thread函数初始化main函数线程的基本信息。init_thread(PCB, name, prio)
// 3. 不需要初始化main函数的线程内核栈
// 4. main函数是正在运行的线程，不在就绪队列中，可以加入到所有线程队列。
~~~

**7.4** 初始化守护线程(系统空闲时运行的线程)；

~~~c
// 通过thread_start（"idle", 10, idle, NULL）创建守护进程
// 执行thread_block函数阻塞自己，被唤醒后执行hlt指令，系统挂起，打到真正的“空闲”；
// 唯一能唤醒处理器的就是外部中断，当外部中断发生后，处理器恢复执行后面的指令。
while(1) {
	thread_block(TASK_BLOCKED);     
    asm volatile ("sti; hlt" : : : "memory");
}
~~~

### 8、实现任务调度器

**8.1** 调度器主要的任务就是读写就绪队列，增删里面的节点，节点是线程PCB中的节点，“相当于线程的PCB”，从队列中将其取出时，可以还原成PCB。

**8.2** 调度器是从就绪队列中取出上处理器的线程PCB，本系统的调度机制很简单，使用轮询调度，也就是先进先出

**8.3** 完整的调度过程需要3部分的配合：①时钟中断的处理函数 ②调度器 schedule ③任务切换函数 switch_to

**8.4**  实现时钟中断处理函数

~~~c
uint32_t ticks;          	// ticks是内核自中断开启以来总共的嘀嗒数
/* 时钟的中断处理函数 */
static void intr_timer_handler(void) {
   	struct task_struct* cur_thread = running_thread();
   	ASSERT(cur_thread->stack_magic == 0x19870916);    	// 检查栈是否溢出
   	cur_thread->elapsed_ticks++;	  					// 记录此线程占用的cpu时间嘀
   	ticks++;	  			//从内核第一次处理时间中断后开始至今的滴哒数,内核态和用户态总共的嘀哒数
   	if (cur_thread->ticks == 0) { 	// 若进程时间片用完就开始调度新的进程上cpu
      	schedule(); 
   	} else {				  		// 将当前进程的时间片-1
      	cur_thread->ticks--;
   	}
}
void timer_init() {
   register_handler(0x20, intr_timer_handler);
}
~~~

**8.5**  实现任务调度 schedule

~~~c
/* 实现任务调度 */
void schedule() {
   	ASSERT(intr_get_status() == INTR_OFF);
   	struct task_struct* cur = running_thread();
    // 若此线程只是cpu时间片到了,将其加入到就绪队列尾
   	if (cur->status == TASK_RUNNING) { 
      	ASSERT(!elem_find(&thread_ready_list, &cur->general_tag));
      	list_append(&thread_ready_list, &cur->general_tag);
        // 重新将当前线程的ticks再重置为其priority
      	cur->ticks = cur->priority;     
      	cur->status = TASK_READY;
   	} else { 
      	// 若此线程需要某事件发生后才能继续上cpu运行,
      	// 不需要将其加入队列,因为当前线程不在就绪队列中。
   	}
   	// 如果就绪队列中没有可运行的任务,就唤醒idle 
   	if (list_empty(&thread_ready_list)) {
      	thread_unblock(idle_thread);
   	}
   	ASSERT(!list_empty(&thread_ready_list));
   	thread_tag = NULL;	  // thread_tag清空
	// 将thread_ready_list队列中的第一个就绪线程弹出,准备将其调度上cpu.
   	thread_tag = list_pop(&thread_ready_list);   
   	struct task_struct* next = elem2entry(struct task_struct, general_tag, thread_tag);
   	next->status = TASK_RUNNING;
   	// 激活任务页表等 
   	process_activate(next);
   	switch_to(cur, next);
}
~~~

### 9、实现任务切换 switch_to

**9.1** 时钟中断处理程序，会使当前的任务执行流第一次发生改变，因此进入中断前要保护第一层的上下文。之后在内核中执行中断处理程序，这属于第2层执行流，当中断处理处理程序调用任务切换函数swicth_to时，当前的中断程序又要被打断，因此要保护好第2层的上下文。

**9.2** 任务调度过程中需要保护好任务两层执行流的上下文。分两部分来完成。第1部分是进入中断时的保护，kernel.S中push了进程的全部寄存器映像。第2部分保护内核环境上下文,根据ABI,除esp外，只保护esi,edi,ebx,ebp这4个寄存器就够了。

![img](https://s2.loli.net/2022/02/14/4cmjMvKnAihDYef.png)

~~~assembly
[bits 32] 
section .text 
global switch_to
switch_to:
    ; 下面3行push,是为了方便看清楚栈的内容
    ; push next
    ; push cur
    ; push eip

    push esi
    push edi
    push ebx
    push ebp 

    mov eax, [esp + 20]     ; [esp + 20] == cur的PCB 
    mov [eax], esp          ; PCB的self_kstack字段，保存0级栈顶指针esp

    mov eax, [esp + 24]     ; [esp + 24] == next的PCB 
    mov esp, [eax]          ; esp栈顶指针指向next的self_kstack, 恢复next的0级栈，0级栈保存PCB  

    pop ebp 
    pop ebx 
    pop edi 
    pop esi 

    ret

~~~

### 10、实现锁

**10.1** 公共资源：是被所有任务共享的一套资源

**10.2** 临界区：程序是通过指令去访问资源的，若多个任务都访问同一公共资源，那么任务中访问公共资源的指令代码组成的区域称为临界区。临界区是程序中访问公共资源的指令代码，即临界区是指令，不是静态的公共资源

**10.3** 互斥：是指某一时刻公共资源只能被一个任务独享，不允许多个任务同时出现在自己的临界区中。

**10.4** 竞争条件：是指多个任务以非互斥的方式方式同时进入临界区，大家对公共资源的访问是以竞争的方式并行进行的，因此公共资源的最终状态依赖于这些任务的临界区中的微操作执行次序。

**10.5** 多线程访问公共资源会产生竞争条件，多个任务同时出现在自己的临界区，为避免产生竞争条件，必须保证任意时刻只能有一个任务处于临界区。

**10.6** 信号量是一种同步机制。信号量是个计数器，它的计数值是自然数，用来记录所积累信号的数量。用P、V操作来表示信号量的减、增，P表示减少，V表示增加。若信号量的初值为1的话，取值就只能为0和1，这是二元信号量可以实现锁，down操作就是获得锁，up操作就是释放锁。

~~~c
// 二元信号量互斥锁，大致流程为：
// 1. 线程A进入临界区先通过down操作获得锁，信号量的值减1后为0
// 2. 后续线程B再进入临界区时也要通过down操作来获得锁，由于信号量的值为0，线程B没有获得锁，便在此信号量上等待（阻塞态）
// 3. 当线程A从临界区出来后执行up操作释放锁，信号量的值加1后为1，之后线程A将线程B唤醒
// 4. 线程B被唤醒后获得了锁，进入临界区。
~~~

~~~c
#ifndef __THREAD_SYNC_H
#define __THREAD_SYNC_H
#include "list.h"
#include "stdint.h"
#include "thread.h"

/* 信号量结构 */
struct semaphore {
   	uint8_t  value;
   	struct   list waiters;
};

/* 锁结构 */
struct lock {
   	struct   task_struct* holder;	    // 锁的持有者
   	struct   semaphore semaphore;	    // 用二元信号量实现锁
   	uint32_t holder_repeat_nr;		    // 锁的持有者重复申请锁的次数
};

void sema_init(struct semaphore* psema, uint8_t value); 
void sema_down(struct semaphore* psema);
void sema_up(struct semaphore* psema);
void lock_init(struct lock* plock);
void lock_acquire(struct lock* plock);
void lock_release(struct lock* plock);
#endif

~~~

~~~c
#include "sync.h"
#include "list.h"
#include "global.h"
#include "debug.h"
#include "interrupt.h"

/* 初始化信号量 */
void sema_init(struct semaphore* psema, uint8_t value) {
   	psema->value = value;       	// 为信号量赋初值
   	list_init(&psema->waiters); 	//初始化信号量的等待队列
}

/* 初始化锁plock */
void lock_init(struct lock *plock) {
   	plock->holder = NULL;
   	plock->holder_repeat_nr = 0;
   	sema_init(&plock->semaphore, 1);  // 信号量初值为1
}

/* 信号量down操作 */
void sema_down(struct semaphore *psema) {
   	enum intr_status old_status = intr_disable(); 	// 关中断来保证原子操作
   	while(psema->value == 0) {	 					// 若value为0,表示已经被别人持有
      	// 当前线程不应该已在信号量的waiters队列中 
      	if (elem_find(&psema->waiters, &running_thread()->general_tag)) {
	 		PANIC("sema_down: thread blocked has been in waiters_list\n");
      	}
		// 若信号量的值等于0,则当前线程把自己加入该锁的等待队列,然后阻塞自己 
      	list_append(&psema->waiters, &running_thread()->general_tag); 
      	thread_block(TASK_BLOCKED);    // 阻塞线程,直到被唤醒
   	}
	// 若value为1或被唤醒后,会执行下面的代码,也就是获得了锁。
   	psema->value--;   
   	intr_set_status(old_status); 	// 恢复之前的中断状态
}

/* 信号量的up操作 */
void sema_up(struct semaphore* psema) {
   	enum intr_status old_status = intr_disable();    
   	if (!list_empty(&psema->waiters)) {
      	struct task_struct* thread_blocked = elem2entry(struct task_struct, general_tag, list_pop(&psema->waiters));
      	thread_unblock(thread_blocked);
   	}
   	psema->value++;  
   	intr_set_status(old_status);
}

/* 获取锁plock */
void lock_acquire(struct lock* plock) {
	// 排除曾经自己已经持有锁但还未将其释放的情况。
   	if (plock->holder != running_thread()) { 
      	sema_down(&plock->semaphore);    	// 对信号量P操作,原子操作
      	plock->holder = running_thread();
      	plock->holder_repeat_nr = 1;
   	} else {
      	plock->holder_repeat_nr++;
   	}
}

/* 释放锁plock */
void lock_release(struct lock* plock) {
   	if (plock->holder_repeat_nr > 1) {
      	plock->holder_repeat_nr--;
      	return;
   	}
   	plock->holder = NULL;	   		// 把锁的持有者置空放在V操作之前
   	plock->holder_repeat_nr = 0;
   	sema_up(&plock->semaphore);	   	// 信号量的V操作,也是原子操作
}

~~~

### 11、线程同步机制

**11.1** 阻塞是线程自己发出的动作，线程自己阻塞自己，并不是别人阻塞的，阻塞是线程主动的行为。已阻塞的线程是由别人来唤醒的，唤醒是被动的。

**11.2** 阻塞的线程是无法运行的，需要锁的持有者线程，它释放了锁后便去唤醒在它后面因获取该锁而阻塞的线程。线程阻塞是线程执行时的动作，因此线程的时间片还没有用完，在唤醒后，线程会继续在剩余的时间片内运行。

~~~c
/* 当前线程将自己阻塞,标志其状态为stat. */
void thread_block(enum task_status stat) {
   	enum intr_status old_status = intr_disable();
   	struct task_struct* cur_thread = running_thread();
   	cur_thread->status = stat; 		// 置其状态为stat 
   	schedule();		      			// 将当前线程换下处理器
	// 待当前线程被解除阻塞后才继续运行下面的intr_set_status
   	intr_set_status(old_status);
}

/* 将线程pthread解除阻塞 */
void thread_unblock(struct task_struct *pthread) {
   	enum intr_status old_status = intr_disable();
   	if (pthread->status != TASK_READY) {
      	if (elem_find(&thread_ready_list, &pthread->general_tag)) {
	 		PANIC("thread_unblock: blocked thread in ready_list\n");
      	}
        // 放到队列的最前面,使其尽快得到调度
      	list_push(&thread_ready_list, &pthread->general_tag);    
      	pthread->status = TASK_READY;
   	} 
   	intr_set_status(old_status);
}
~~~

**11.3** 实现thread_yield，它的功能是主动把CPU处理器使用权让处理，它与thread_block的区别是thread_yield执行后任务状态是TASK_READY,即让出CPU后，会被加入到就绪队列中，下次还能继续被调度器调度执行。thread_block执行后的状态为TASK_BLOCKED,需要被唤醒后才能加入到就绪队列。

~~~c
/* 主动让出cpu,换其它线程运行 */
void thread_yield(void) {
   	struct task_struct* cur = running_thread();   
   	enum intr_status old_status = intr_disable();
   	list_append(&thread_ready_list, &cur->general_tag);
   	cur->status = TASK_READY;
   	schedule();
   	intr_set_status(old_status);
}
~~~

### 12、实现线程休眠函数

**12.1** 硬盘和CPU是相互独立的个体，它们各自并行执行，但由于硬盘是低速设备，其在处理请求时往往消耗很长时间，为避免浪费CPU资源，在等待硬盘操作的过程最好把CPU主动让出来，让CPU去执行其他任务，故在timer模块定义休眠函数，让线程睡一会。

~~~c
#define mil_seconds_per_intr (1000 / IRQ0_FREQUENCY)

// 以tick为单位的sleep,任何时间形式的sleep会转换此ticks形式
static void ticks_to_sleep(uint32_t sleep_ticks) {
	uint32_t start_tick = ticks;
    // 若间隔的ticks数不够便让出cpu
   	while (ticks - start_tick < sleep_ticks) {	   
      	thread_yield();
   	}
}

// 以毫秒为单位的sleep   1秒= 1000毫秒
void mtime_sleep(uint32_t m_seconds) {
  	uint32_t sleep_ticks = DIV_ROUND_UP(m_seconds, mil_seconds_per_intr);
  	ticks_to_sleep(sleep_ticks); 
}
~~~

### 13、销毁线程

~~~c
/* thread_exit函数功能：回收thread的PCB和页表，并将其从调度队列中去除 */
// 输入参数 thread_over：待退出的任务
// 输入参数：是否调度标记

/* 回收thread_over的pcb和页表,并将其从调度队列中去除 */
void thread_exit(struct task_struct *thread_over, bool need_schedule) { 
   	intr_disable(); 				// 要保证schedule在关中断情况下调用
   	thread_over->status = TASK_DIED;
   	// 如果thread_over不是当前线程,就有可能还在就绪队列中,将其从中删除 
   	if (elem_find(&thread_ready_list, &thread_over->general_tag)) {
      	list_remove(&thread_over->general_tag);
   	}
   	if (thread_over->pgdir) {     // 如是进程,回收进程的页表
      	mfree_page(PF_KERNEL, thread_over->pgdir, 1);
   	}
   	// 从all_thread_list中去掉此任务 
   	list_remove(&thread_over->all_list_tag);
   	// 回收pcb所在的页,主线程的pcb不在堆中,跨过
   	if (thread_over != main_thread) {
      	mfree_page(PF_KERNEL, thread_over, 1);
   	}
   	// 归还pid 
   	release_pid(thread_over->pid);
   	// 如果需要下一轮调度则主动调用schedule 
   	if (need_schedule) {
      	schedule();
      	PANIC("thread_exit: should not be here\n");
   	}
}
~~~

### 14、实现PS打印任务相关信息

~~~c
 /* 打印任务列表 */
void sys_ps(void) {
   	char* ps_title = "PID            PPID           STAT           TICKS          COMMAND\n";
   	sys_write(stdout_no, ps_title, strlen(ps_title)); 		// 打印标题
   	list_traversal(&thread_all_list, elem2thread_info, 0); 	// 打印所有任务的信息
}
~~~

### 15、 thread.c代码实现

~~~c
// 头文件 thread.h

#ifndef __THREAD_THREAD_H
#define __THREAD_THREAD_H

#include "stdint.h"
#include "list.h"
#include "bitmap.h"
#include "memory.h"

#define TASK_NAME_LEN 16
#define MAX_FILES_OPEN_PER_PROC 8
/* 自定义通用函数类型,它将在很多线程函数中做为形参类型 */
typedef void thread_func(void*);
typedef int16_t pid_t;

/* 进程或线程的状态 */
enum task_status {
   	TASK_RUNNING,
   	TASK_READY,
   	TASK_BLOCKED,
   	TASK_WAITING,
   	TASK_HANGING,
   	TASK_DIED
};

/***********   中断栈intr_stack   ***********
 * 此结构用于中断发生时保护程序(线程或进程)的上下文环境:
 * 进程或线程被外部中断或软中断打断时,会按照此结构压入上下文
 * 寄存器,  intr_exit中的出栈操作是此结构的逆操作
 * 此栈在线程自己的内核栈中位置固定,所在页的最顶端
********************************************/
struct intr_stack {
	uint32_t vec_no;	 	// kernel.S 宏VECTOR中push %1压入的中断号
	uint32_t edi;
	uint32_t esi;
    uint32_t ebp;
    uint32_t esp_dummy;	 	// 虽然pushad把esp也压入,但esp是不断变化的,所以会被popad忽略
    uint32_t ebx;
    uint32_t edx;
    uint32_t ecx;
    uint32_t eax;
    uint32_t gs;
    uint32_t fs;
    uint32_t es;
    uint32_t ds;
	// 以下由cpu从低特权级进入高特权级时压入 
    uint32_t err_code;		// err_code会被压入在eip之后
    void (*eip) (void);
    uint32_t cs;
    uint32_t eflags;
    void* esp;
    uint32_t ss;
};

/***********  线程栈thread_stack  ***********
 * 线程自己的栈,用于存储线程中待执行的函数
 * 此结构在线程自己的内核栈中位置不固定,
 * 用在switch_to时保存线程环境。
 * 实际位置取决于实际运行情况。
 ******************************************/
struct thread_stack {
  	uint32_t ebp;
   	uint32_t ebx;
	uint32_t edi;
   	uint32_t esi;
	// 线程第一次执行时,eip指向待调用的函数kernel_thread 
	// 其它时候,eip是指向switch_to的返回地址
   	void (*eip) (thread_func* func, void* func_arg);
	//	以下仅供第一次被调度上cpu时使用
   	void (*unused_retaddr); 	// 参数unused_ret只为占位置充数为返回地址
   	thread_func* function;   	// 由Kernel_thread所调用的函数名
   	void* func_arg;    			// 由Kernel_thread所调用的函数所需的参数
};

/* 进程或线程的pcb,程序控制块 */
struct task_struct {
   	uint32_t* self_kstack;	 	// 各内核线程都用自己的内核栈
   	pid_t pid;
   	enum task_status status;
   	char name[TASK_NAME_LEN];
   	uint8_t priority;
   	uint8_t ticks;	   			// 每次在处理器上执行的时间嘀嗒数
   	uint32_t elapsed_ticks; 	// 此任务自上cpu运行后至今占用了多少嘀嗒数
   	struct list_elem general_tag; 	// 用于线程在一般的队列中的结点			    
   	struct list_elem all_list_tag; 	// 用于线程队列thread_all_list中的结点
   	uint32_t* pgdir;              	// 进程自己页表的虚拟地址
   	struct virtual_addr userprog_vaddr;   			// 用户进程的虚拟地址
   	struct mem_block_desc u_block_desc[DESC_CNT];   // 用户进程内存块描述符
   	int32_t fd_table[MAX_FILES_OPEN_PER_PROC];		// 已打开文件数组
   	uint32_t cwd_inode_nr;	 						// 进程所在的工作目录的inode编号
   	pid_t parent_pid;		 						// 父进程pid
   	int8_t  exit_status;         					// 进程结束时自己调用exit传入的参数
   	uint32_t stack_magic;	 						// 栈的边界标记,用于检测栈的溢出
};

extern struct list thread_ready_list;
extern struct list thread_all_list;

void thread_create(struct task_struct* pthread, thread_func function, void* func_arg);
void init_thread(struct task_struct* pthread, char* name, int prio);
struct task_struct* thread_start(char* name, int prio, thread_func function, void* func_arg);
struct task_struct* running_thread(void);
void schedule(void);
void thread_init(void);
void thread_block(enum task_status stat);
void thread_unblock(struct task_struct* pthread);
void thread_yield(void);
pid_t fork_pid(void);
void sys_ps(void);
void thread_exit(struct task_struct* thread_over, bool need_schedule);
struct task_struct* pid2thread(int32_t pid);
void release_pid(pid_t pid);
#endif

~~~

~~~c
// thread.c文件

#include "thread.h"
#include "stdint.h"
#include "string.h"
#include "global.h"
#include "debug.h"
#include "interrupt.h"
#include "print.h"
#include "memory.h"
#include "process.h"
#include "stdio.h"
#include "console.h"
#include "fs.h"
#include "file.h"

/* pid的位图,最大支持1024个pid */
uint8_t pid_bitmap_bits[128] = {0};

/* pid池 */
struct pid_pool {
   	struct bitmap pid_bitmap; 	// pid位图
   	uint32_t pid_start;	      	// 起始pid
   	struct lock pid_lock;      	// 分配pid锁
} pid_pool;

struct task_struct *main_thread;    	// 主线程PCB
struct task_struct *idle_thread;    	// idle线程
struct list thread_ready_list;	    	// 就绪队列
struct list thread_all_list;	    	// 所有任务队列
static struct list_elem *thread_tag;	// 用于保存队列中的线程结点

extern void switch_to(struct task_struct *cur, struct task_struct *next);
extern void init(void);

/* 系统空闲时运行的线程 */
static void idle(void *arg UNUSED) {
    while(1) {
      	thread_block(TASK_BLOCKED);     
      	//执行hlt时必须要保证目前处在开中断的情况下
      	asm volatile ("sti; hlt" : : : "memory");
   	}
}

/* 获取当前线程pcb指针 */
struct task_struct* running_thread() {
   	uint32_t esp; 
   	asm ("mov %%esp, %0" : "=g" (esp));
  	// 取esp整数部分即pcb起始地址
   	return (struct task_struct*)(esp & 0xfffff000);
}

/* 由kernel_thread去执行function(func_arg) */
static void kernel_thread(thread_func* function, void* func_arg) {
	// 开中断,避免后面的时钟中断被屏蔽,而无法调度其它线程 
   	intr_enable();
   	function(func_arg); 
}

/* 初始化pid池 */
static void pid_pool_init(void) { 
   	pid_pool.pid_start = 1;
   	pid_pool.pid_bitmap.bits = pid_bitmap_bits;
   	pid_pool.pid_bitmap.btmp_bytes_len = 128;
   	bitmap_init(&pid_pool.pid_bitmap);
   	lock_init(&pid_pool.pid_lock);
}

/* 分配pid */
static pid_t allocate_pid(void) {
   	lock_acquire(&pid_pool.pid_lock);
    
   	int32_t bit_idx = bitmap_scan(&pid_pool.pid_bitmap, 1);
   	bitmap_set(&pid_pool.pid_bitmap, bit_idx, 1);
    
   	lock_release(&pid_pool.pid_lock);
   	return (bit_idx + pid_pool.pid_start);
}

/* 释放pid */
void release_pid(pid_t pid) {
   	lock_acquire(&pid_pool.pid_lock);
    
   	int32_t bit_idx = pid - pid_pool.pid_start;
   	bitmap_set(&pid_pool.pid_bitmap, bit_idx, 0);
    
   	lock_release(&pid_pool.pid_lock);
}

/* fork进程时为其分配pid,因为allocate_pid已经是静态的,别的文件无法调用.
不想改变函数定义了,故定义fork_pid函数来封装一下。*/
pid_t fork_pid(void) {
   	return allocate_pid();
}

/* 初始化线程栈thread_stack,将待执行的函数和参数放到thread_stack中相应的位置 */
void thread_create(struct task_struct* pthread, thread_func function, void* func_arg) {
   	// 预留中断使用栈的空间和线程栈空间
   	pthread->self_kstack -= sizeof(struct intr_stack);
   	pthread->self_kstack -= sizeof(struct thread_stack);
    
   	struct thread_stack* kthread_stack = (struct thread_stack*)pthread->self_kstack;
   	kthread_stack->eip = kernel_thread;
   	kthread_stack->function = function;
   	kthread_stack->func_arg = func_arg;
   	kthread_stack->ebp = kthread_stack->ebx = kthread_stack->esi = kthread_stack->edi = 0;
}

/* 初始化线程基本信息 */
void init_thread(struct task_struct* pthread, char* name, int prio) {
   	memset(pthread, 0, sizeof(*pthread));
   	pthread->pid = allocate_pid();
   	strcpy(pthread->name, name);
   	if (pthread == main_thread) {
		// 把main函数也封装成一个线程,并且它一直是运行的
      	pthread->status = TASK_RUNNING;
   	} else {
      	pthread->status = TASK_READY;
   	}
	// self_kstack是线程自己在内核态下使用的栈顶地址
   	pthread->self_kstack = (uint32_t*)((uint32_t)pthread + PG_SIZE);
    
  	pthread->priority = prio;
   	pthread->ticks = prio;
   	pthread->elapsed_ticks = 0;
   	pthread->pgdir = NULL;
   	// 标准输入输出先空出来 
   	pthread->fd_table[0] = 0;
   	pthread->fd_table[1] = 1;
   	pthread->fd_table[2] = 2;
   	// 其余的全置为-1 
   	uint8_t fd_idx = 3;
   	while (fd_idx < MAX_FILES_OPEN_PER_PROC) {
      	pthread->fd_table[fd_idx] = -1;
      	fd_idx++;
   	}
    
   	pthread->cwd_inode_nr = 0;	    	// 以根目录做为默认工作路径
   	pthread->parent_pid = -1;        	// -1表示没有父进程
   	pthread->stack_magic = 0x19870916; 	// 自定义的魔数
}

/* 创建一优先级为prio的线程,线程名为name,线程所执行的函数是function(func_arg) */
struct task_struct* thread_start(char* name, int prio, thread_func function, void* func_arg) {
	// pcb都位于内核空间,包括用户进程的pcb也是在内核空间
   	struct task_struct* thread = get_kernel_pages(1);
   	init_thread(thread, name, prio);
   	thread_create(thread, function, func_arg);
	// 加入就绪线程队列 
   	ASSERT(!elem_find(&thread_ready_list, &thread->general_tag));
   	list_append(&thread_ready_list, &thread->general_tag);
	// 加入全部线程队列 
   	ASSERT(!elem_find(&thread_all_list, &thread->all_list_tag));
   	list_append(&thread_all_list, &thread->all_list_tag);
   	return thread;
}

/* 将kernel中的main函数完善为主线程 */
static void make_main_thread(void) {
	// main线程早已运行,在loader.S中进入内核时的mov esp,0xc009f000,
	// 为其预留了tcb,地址为0xc009e000,因此不需要通过get_kernel_page另分配一页
   	main_thread = running_thread();
   	init_thread(main_thread, "main", 31);
	// main函数是当前线程,当前线程不在thread_ready_list中,
 	//* 所以只将其加在thread_all_list中. 
   	ASSERT(!elem_find(&thread_all_list, &main_thread->all_list_tag));
   	list_append(&thread_all_list, &main_thread->all_list_tag);
}

/* 实现任务调度 */
void schedule() {
   	ASSERT(intr_get_status() == INTR_OFF);
   	struct task_struct* cur = running_thread();
    // 若此线程只是cpu时间片到了,将其加入到就绪队列尾
   	if (cur->status == TASK_RUNNING) { 
      	ASSERT(!elem_find(&thread_ready_list, &cur->general_tag));
      	list_append(&thread_ready_list, &cur->general_tag);
        // 重新将当前线程的ticks再重置为其priority
      	cur->ticks = cur->priority;     
      	cur->status = TASK_READY;
   	} else { 
      	// 若此线程需要某事件发生后才能继续上cpu运行,
      	// 不需要将其加入队列,因为当前线程不在就绪队列中。
   	}

   	// 如果就绪队列中没有可运行的任务,就唤醒idle 
   	if (list_empty(&thread_ready_list)) {
      	thread_unblock(idle_thread);
   	}

   	ASSERT(!list_empty(&thread_ready_list));
   	thread_tag = NULL;	  // thread_tag清空
	// 将thread_ready_list队列中的第一个就绪线程弹出,准备将其调度上cpu.
   	thread_tag = list_pop(&thread_ready_list);   
   	struct task_struct* next = elem2entry(struct task_struct, general_tag, thread_tag);
   	next->status = TASK_RUNNING;
   	// 激活任务页表等 
   	process_activate(next);
   	switch_to(cur, next);
}

/* 当前线程将自己阻塞,标志其状态为stat. */
void thread_block(enum task_status stat) {
   	ASSERT(((stat == TASK_BLOCKED) || (stat == TASK_WAITING) || (stat == TASK_HANGING)));
   	enum intr_status old_status = intr_disable();
   	struct task_struct* cur_thread = running_thread();
   	cur_thread->status = stat; 		// 置其状态为stat 
   	schedule();		      			// 将当前线程换下处理器
	// 待当前线程被解除阻塞后才继续运行下面的intr_set_status
   	intr_set_status(old_status);
}

/* 将线程pthread解除阻塞 */
void thread_unblock(struct task_struct* pthread) {
   	enum intr_status old_status = intr_disable();
   	ASSERT(((pthread->status == TASK_BLOCKED) || (pthread->status == TASK_WAITING) || (pthread->status == TASK_HANGING)));
   	if (pthread->status != TASK_READY) {
      	ASSERT(!elem_find(&thread_ready_list, &pthread->general_tag));
      	if (elem_find(&thread_ready_list, &pthread->general_tag)) {
	 		PANIC("thread_unblock: blocked thread in ready_list\n");
      	}
        // 放到队列的最前面,使其尽快得到调度
      	list_push(&thread_ready_list, &pthread->general_tag);    
      	pthread->status = TASK_READY;
   	} 
   	intr_set_status(old_status);
}

/* 主动让出cpu,换其它线程运行 */
void thread_yield(void) {
   	struct task_struct* cur = running_thread();   
   	enum intr_status old_status = intr_disable();
   	ASSERT(!elem_find(&thread_ready_list, &cur->general_tag));
   	list_append(&thread_ready_list, &cur->general_tag);
   	cur->status = TASK_READY;
   	schedule();
   	intr_set_status(old_status);
}


/* 以填充空格的方式输出buf */
static void pad_print(char* buf, int32_t buf_len, void* ptr, char format) {
   	memset(buf, 0, buf_len);
   	uint8_t out_pad_0idx = 0;
   	switch(format) {
      	case 's':
	 		out_pad_0idx = sprintf(buf, "%s", ptr);
	 		break;
      	case 'd':
	 		out_pad_0idx = sprintf(buf, "%d", *((int16_t*)ptr));
      	case 'x':
	 		out_pad_0idx = sprintf(buf, "%x", *((uint32_t*)ptr));
   	}
    // 以空格填充
   	while(out_pad_0idx < buf_len) { 
      	buf[out_pad_0idx] = ' ';
      	out_pad_0idx++;
   	}
   	sys_write(stdout_no, buf, buf_len - 1);
}

/* 用于在list_traversal函数中的回调函数,用于针对线程队列的处理 */
static bool elem2thread_info(struct list_elem* pelem, int arg UNUSED) {
   	struct task_struct* pthread = elem2entry(struct task_struct, all_list_tag, pelem);
   	char out_pad[16] = {0};

   	pad_print(out_pad, 16, &pthread->pid, 'd');
   	if (pthread->parent_pid == -1) {
      	pad_print(out_pad, 16, "NULL", 's');
   	} else { 
      	pad_print(out_pad, 16, &pthread->parent_pid, 'd');
   	}
   	switch (pthread->status) {
      	case 0:
	 		pad_print(out_pad, 16, "RUNNING", 's');
	 		break;
      	case 1:
	 		pad_print(out_pad, 16, "READY", 's');
	 		break;
      	case 2:
	 		pad_print(out_pad, 16, "BLOCKED", 's');
	 		break;
      	case 3:
	 		pad_print(out_pad, 16, "WAITING", 's');
	 		break;
      	case 4:
	 		pad_print(out_pad, 16, "HANGING", 's');
	 		break;
      	case 5:
	 		pad_print(out_pad, 16, "DIED", 's');
   	}
   	pad_print(out_pad, 16, &pthread->elapsed_ticks, 'x');

   	memset(out_pad, 0, 16);
   	ASSERT(strlen(pthread->name) < 17);
   	memcpy(out_pad, pthread->name, strlen(pthread->name));
   	strcat(out_pad, "\n");
   	sys_write(stdout_no, out_pad, strlen(out_pad));
    // 返回false是为了迎合主调函数list_traversal,只有回调函数返回false时才会继续调用此函数
   	return false;	
}

 /* 打印任务列表 */
void sys_ps(void) {
   	char* ps_title = "PID            PPID           STAT           TICKS          COMMAND\n";
   	sys_write(stdout_no, ps_title, strlen(ps_title));
   	list_traversal(&thread_all_list, elem2thread_info, 0);
}

/* 回收thread_over的pcb和页表,并将其从调度队列中去除 */
void thread_exit(struct task_struct* thread_over, bool need_schedule) {
   	// 要保证schedule在关中断情况下调用 
   	intr_disable();
   	thread_over->status = TASK_DIED;

   	// 如果thread_over不是当前线程,就有可能还在就绪队列中,将其从中删除 
   	if (elem_find(&thread_ready_list, &thread_over->general_tag)) {
      	list_remove(&thread_over->general_tag);
   	}
   	if (thread_over->pgdir) {     // 如是进程,回收进程的页表
      	mfree_page(PF_KERNEL, thread_over->pgdir, 1);
   	}

   	// 从all_thread_list中去掉此任务 
   	list_remove(&thread_over->all_list_tag);
   	// 回收pcb所在的页,主线程的pcb不在堆中,跨过
   	if (thread_over != main_thread) {
      	mfree_page(PF_KERNEL, thread_over, 1);
   	}
   	// 归还pid 
   	release_pid(thread_over->pid);
   	// 如果需要下一轮调度则主动调用schedule 
   	if (need_schedule) {
      	schedule();
      	PANIC("thread_exit: should not be here\n");
   	}
}

/* 比对任务的pid */
static bool pid_check(struct list_elem* pelem, int32_t pid) {
   	struct task_struct* pthread = elem2entry(struct task_struct, all_list_tag, pelem);
   	if (pthread->pid == pid) {
      	return true;
   	}
   	return false;
}

/* 根据pid找pcb,若找到则返回该pcb,否则返回NULL */
struct task_struct* pid2thread(int32_t pid) {
   	struct list_elem* pelem = list_traversal(&thread_all_list, pid_check, pid);
   	if (pelem == NULL) {
      	return NULL;
   	}
   	struct task_struct* thread = elem2entry(struct task_struct, all_list_tag, pelem);
   	return thread;
}

/* 初始化线程环境 */
void thread_init(void) {
   	put_str("thread_init start\n");
   	list_init(&thread_ready_list);
   	list_init(&thread_all_list);
   	pid_pool_init();
 	// 先创建第一个用户进程:init 
    // 放在第一个初始化,这是第一个进程,init进程的pid为1
   	process_execute(init, "init");         
	// 将当前main函数创建为线程 
   	make_main_thread();
   	// 创建idle线程 
   	idle_thread = thread_start("idle", 10, idle, NULL);
   	put_str("thread_init done\n");
}


~~~



